# Uses Chatterbox-TTS for voice cloning
# Uses the lipsync library for lipsyncing
# Uses Mediapipe's rembg to remove background from the video and its first frame to isolate the person of interest and place them on a black background.
# Uses Ollama to implement Microsoft's Phi 4 LLM AI model to enable conversation
# Features a custom-trained ML model to determine if a prompt requires internet consultation. 
# If a question requires internet consultation, then the program will provide some relevant search results as context to the AI model.
# Displays unique icons to notify user which stage of processing the program is currently in.
# Sends signals via PySerial to a 360 degree camera to enable face-tracking, so that the microphone attached on top is always pointed towards the user.

# Notes:
# - Follow the dependency installation instructions in requirements.txt
# - You will need to run 'sudo apt remove --purge brltty' for the conference camera to connect properly on Ubuntu 24.04 LTS
# - You will need to run 'sudo usermod -a -G dialout $USER' and then log out then log back in (or restart) for the program to access the conference camera


import torch
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
import os
import subprocess
import sys
import time
from flask import Flask, request, render_template_string, jsonify
from werkzeug.utils import secure_filename
import shutil
import threading
import speech_recognition as sr
import requests
import json
import numpy as np
import base64
from PIL import Image
from io import BytesIO
import mediapipe as mp
from datetime import date
import pygame
import serial
import re
import logging
import cv2

# --- NEW: Internet Search Dependencies ---
import random
import urllib.parse
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# --- Configuration ---
UPLOAD_FOLDER = "uploads"
VOICE_SAMPLES_DIR = os.path.join(UPLOAD_FOLDER, "voice_samples")
INPUT_VIDEO_DIR = os.path.join(UPLOAD_FOLDER, "video")
OUTPUT_WAV_PATH = "generated_speech.wav" # Generated by TTS
TEMP_DIR = "temp"

# Audio processing configuration
EXTRACTED_AUDIO_PATH = "extracted_audio.wav"
ISOLATED_VOICE_PATH = "isolated_voice.wav"

# --- NEW: Interactive Configuration ---
# Make sure you create an 'icons' folder and add these files.
MIC_ICON_PATH = "icons/mic_on.png"
THINKING_ICON_PATH = "icons/thinking.png"

# --- NEW: Ollama Configuration ---
OLLAMA_API_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "phi4:latest"  # The model you want to use

# --- System Prompt Configuration ---
BASE_SYSTEM_PROMPT = "Keep your responses quite short (aim for less than 7 seconds of verbal speech) but still friendly. You are a hologram clone of a human, so act human, don't act robotic. Additional details may or may not be provided for who you are."  # Always included base prompt

# --- OpenCV Window Name ---
DISPLAY_WINDOW_NAME = "PixelHolo Clone"

# --- Flask App Setup ---
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 1024 * 1024 * 1024 # 1 GB Upload Limit

# Global variables
uploaded_voice_samples = []
uploaded_input_video = None
setup_complete = False
conversation_history = []  # Store conversation for context
user_system_prompt = None  # Store user's custom system prompt

# Face tracking globals
ser = None
tracking_paused = False
camera_lock = threading.Lock()
latest_frame = None
camera_cap = None
tracking_active = True

# --- SERIAL CONFIG ---
POSSIBLE_PORTS = ['/dev/ttyUSB0']
BAUD = 9600
PAN_SPEED = 0x18
TILT_SPEED = 0x18

def setup_upload_directories():
    """Ensures upload directories exist and are clean on startup."""
    if os.path.exists(UPLOAD_FOLDER):
        shutil.rmtree(UPLOAD_FOLDER)
    os.makedirs(VOICE_SAMPLES_DIR, exist_ok=True)
    os.makedirs(INPUT_VIDEO_DIR, exist_ok=True)
    os.makedirs(TEMP_DIR, exist_ok=True)
    os.makedirs("icons", exist_ok=True) # Ensure icons directory exists

@app.route('/')
def index():
    """Renders the main upload page with modern UI and camera option."""
    return render_template_string("""
        <!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Voice Clone Generator</title>
            <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
            <style>
                * {
                    margin: 0;
                    padding: 0;
                    box-sizing: border-box;
                }
                
                body {
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    min-height: 100vh;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    padding: 20px;
                }
                
                .container {
                    background: rgba(255, 255, 255, 0.95);
                    backdrop-filter: blur(10px);
                    border-radius: 20px;
                    padding: 40px;
                    box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
                    max-width: 650px;
                    width: 100%;
                    border: 1px solid rgba(255, 255, 255, 0.2);
                }
                
                h1 {
                    color: #2d3748;
                    font-size: 2.5rem;
                    font-weight: 700;
                    text-align: center;
                    margin-bottom: 10px;
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    -webkit-background-clip: text;
                    -webkit-text-fill-color: transparent;
                    background-clip: text;
                }
                
                .subtitle {
                    text-align: center;
                    color: #718096;
                    font-size: 1.1rem;
                    margin-bottom: 40px;
                    font-weight: 400;
                }
                
                .field-container {
                    margin-bottom: 30px;
                }
                
                .field-label {
                    display: flex;
                    align-items: center;
                    font-weight: 600;
                    color: #2d3748;
                    margin-bottom: 12px;
                    font-size: 1.1rem;
                }
                
                .field-label i {
                    margin-right: 10px;
                    color: #667eea;
                    width: 20px;
                }
                
                .photo-options {
                    display: flex;
                    gap: 15px;
                    margin-bottom: 15px;
                }
                
                .photo-option-btn {
                    flex: 1;
                    padding: 12px 20px;
                    background: linear-gradient(135deg, #f7fafc, #edf2f7);
                    border: 2px solid #cbd5e0;
                    border-radius: 12px;
                    cursor: pointer;
                    transition: all 0.3s ease;
                    text-align: center;
                    font-weight: 600;
                    color: #4a5568;
                }
                
                .photo-option-btn:hover {
                    background: linear-gradient(135deg, #edf2f7, #e2e8f0);
                    border-color: #667eea;
                    transform: translateY(-2px);
                }
                
                .photo-option-btn.active {
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    color: white;
                    border-color: #667eea;
                }
                
                .photo-option-btn i {
                    margin-right: 8px;
                }
                
                .file-input-wrapper {
                    position: relative;
                    display: inline-block;
                    width: 100%;
                }
                
                .file-input {
                    position: absolute;
                    opacity: 0;
                    width: 100%;
                    height: 100%;
                    cursor: pointer;
                }
                
                .file-input-display {
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    background: linear-gradient(135deg, #f7fafc, #edf2f7);
                    border: 2px dashed #cbd5e0;
                    border-radius: 12px;
                    padding: 30px 20px;
                    transition: all 0.3s ease;
                    cursor: pointer;
                    text-align: center;
                    min-height: 100px;
                }
                
                .file-input-display:hover {
                    background: linear-gradient(135deg, #edf2f7, #e2e8f0);
                    border-color: #667eea;
                    transform: translateY(-2px);
                }
                
                .file-input-display i {
                    font-size: 2rem;
                    color: #667eea;
                    margin-bottom: 10px;
                }
                
                .file-input-text {
                    color: #4a5568;
                    font-weight: 500;
                }
                
                .file-input-subtext {
                    color: #718096;
                    font-size: 0.9rem;
                    margin-top: 5px;
                }
                
                .camera-container {
                    display: none;
                    text-align: center;
                    background: linear-gradient(135deg, #f7fafc, #edf2f7);
                    border: 2px solid #cbd5e0;
                    border-radius: 12px;
                    padding: 20px;
                    margin-bottom: 15px;
                }
                
                .camera-container.active {
                    display: block;
                }
                
                #cameraVideo {
                    width: 100%;
                    max-width: 400px;
                    height: 300px;
                    background: #000;
                    border-radius: 8px;
                    margin-bottom: 15px;
                }
                
                #capturedPhoto {
                    width: 100%;
                    max-width: 400px;
                    max-height: 300px;
                    border-radius: 8px;
                    margin-bottom: 15px;
                    display: none;
                }
                
                .camera-controls {
                    display: flex;
                    gap: 10px;
                    justify-content: center;
                    flex-wrap: wrap;
                }
                
                .camera-btn {
                    padding: 10px 20px;
                    border: none;
                    border-radius: 8px;
                    cursor: pointer;
                    font-weight: 600;
                    transition: all 0.3s ease;
                    font-size: 0.9rem;
                }
                
                .camera-btn.primary {
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    color: white;
                }
                
                .camera-btn.secondary {
                    background: #e2e8f0;
                    color: #4a5568;
                }
                
                .camera-btn:hover {
                    transform: translateY(-2px);
                }
                
                .camera-btn:disabled {
                    opacity: 0.5;
                    cursor: not-allowed;
                    transform: none;
                }
                
                textarea {
                    width: 100%;
                    min-height: 120px;
                    padding: 20px;
                    border: 2px solid #e2e8f0;
                    border-radius: 12px;
                    font-family: inherit;
                    font-size: 1rem;
                    resize: vertical;
                    transition: all 0.3s ease;
                    background: rgba(255, 255, 255, 0.8);
                }
                
                textarea:focus {
                    outline: none;
                    border-color: #667eea;
                    box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
                    background: white;
                }
                
                textarea::placeholder {
                    color: #a0aec0;
                    font-style: italic;
                }
                
                .submit-btn {
                    width: 100%;
                    background: linear-gradient(135deg, #667eea, #764ba2);
                    color: white;
                    padding: 18px 30px;
                    border: none;
                    border-radius: 12px;
                    font-size: 1.2rem;
                    font-weight: 600;
                    cursor: pointer;
                    transition: all 0.3s ease;
                    text-transform: uppercase;
                    letter-spacing: 1px;
                    box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
                }
                
                .submit-btn:hover {
                    transform: translateY(-3px);
                    box-shadow: 0 15px 30px rgba(102, 126, 234, 0.4);
                }
                
                .submit-btn:active {
                    transform: translateY(-1px);
                }
                
                .submit-btn:disabled {
                    opacity: 0.5;
                    cursor: not-allowed;
                    transform: none;
                }
                
                .info-box {
                    background: linear-gradient(135deg, #ebf8ff, #bee3f8);
                    border-left: 4px solid #3182ce;
                    padding: 20px;
                    border-radius: 8px;
                    margin-bottom: 30px;
                    color: #2a69ac;
                    font-size: 0.95rem;
                    line-height: 1.6;
                }
                
                .info-box i {
                    margin-right: 8px;
                    color: #3182ce;
                }
                
                .hidden {
                    display: none !important;
                }
                
                @media (max-width: 768px) {
                    .container {
                        padding: 25px;
                        margin: 10px;
                    }
                    
                    h1 {
                        font-size: 2rem;
                    }
                    
                    .photo-options {
                        flex-direction: column;
                    }
                    
                    .file-input-display {
                        padding: 20px 15px;
                        min-height: 80px;
                    }
                    
                    .file-input-display i {
                        font-size: 1.5rem;
                    }
                    
                    .camera-controls {
                        flex-direction: column;
                        align-items: center;
                    }
                    
                    .camera-btn {
                        width: 100%;
                        max-width: 200px;
                    }
                }
                
                /* Animation for page load */
                .container {
                    animation: fadeInUp 0.8s ease-out;
                }
                
                @keyframes fadeInUp {
                    from {
                        opacity: 0;
                        transform: translateY(30px);
                    }
                    to {
                        opacity: 1;
                        transform: translateY(0);
                    }
                }
                
                /* File input selected state */
                .file-selected .file-input-display {
                    background: linear-gradient(135deg, #e6fffa, #b2f5ea);
                    border-color: #38b2ac;
                    color: #234e52;
                }
                
                .file-selected .file-input-display i {
                    color: #38b2ac;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1><i class="fas fa-robot"></i> PixelHolo Cloner</h1>
                <p class="subtitle">Create your personalized AI avatar with custom voice and personality</p>
                
                <div class="info-box">
                    <i class="fas fa-info-circle"></i>
                    <strong>Quick Start:</strong> Upload clear voice samples (WAV format) and provide a high-quality photo. 
                    You can either upload an existing photo or take a new one with your camera!
                </div>
                
                <form method="post" action="/upload_files" enctype="multipart/form-data" id="uploadForm">
                    <div class="info-box">
                        <i class="fas fa-magic"></i>
                        <strong>AI Voice Extraction:</strong> The system will automatically extract and isolate your voice from the uploaded video using advanced audio processing techniques.
                    </div>
                    
                    <!-- Video Section -->
                    <div class="field-container">
                        <label class="field-label">
                            <i class="fas fa-video"></i>
                            Your Video
                        </label>
                        
                        <div class="file-input-wrapper" id="upload-option">
                            <input type="file" name="input_video" accept="video/*" 
                                class="file-input" onchange="handleFileSelect(this, 'video-display')">
                            <div class="file-input-display" id="video-display">
                                <div>
                                    <i class="fas fa-video"></i>
                                    <div class="file-input-text">Click to upload your video</div>
                                    <div class="file-input-subtext">MP4, AVI, MOV, or other video formats</div>
                                </div>
                            </div>
                        </div>
                        
                        <input type="hidden" name="camera_photo" id="cameraPhotoData">
                    </div>
                    
                    <div class="field-container">
                        <label class="field-label">
                            <i class="fas fa-brain"></i>
                            Personality & Identity
                        </label>
                        <textarea name="system_prompt" 
                                  placeholder="Describe who your clone should be... Are they a friendly teacher, a witty comedian, a wise mentor? Include name, personality traits, speaking style, interests, and any quirks that make them unique. The more detail you provide, the more authentic your clone will feel!"></textarea>
                    </div>
                    
                    <input type="submit" value="🚀 Create My Clone" class="submit-btn" id="submitBtn">
                </form>
            </div>
            
            <script>
                let stream = null;
                let photoMode = 'upload'; // 'upload' or 'camera'
                let capturedPhotoData = null;
                
                function handleFileSelect(input, displayId) {
                    const display = document.getElementById(displayId);
                    const files = input.files;
                    
                    if (files.length > 0) {
                        display.classList.add('file-selected');
                        
                        // Force update the entire content
                        const fileName = files.length === 1 ? files[0].name : `${files.length} files selected`;
                        const fileMessage = files.length === 1 ? 'File selected successfully!' : 'Files selected successfully!';
                        
                        display.innerHTML = `
                            <div>
                                <i class="fas fa-check-circle"></i>
                                <div class="file-input-text">${fileName}</div>
                                <div class="file-input-subtext">${fileMessage}</div>
                            </div>
                        `;
                    }
                }
                                  
                function validateFileSize(input) {
                    const maxSize = 1024 * 1024 * 1024; // 1 GB in bytes
                    if (input.files && input.files[0] && input.files[0].size > maxSize) {
                        alert('Video file is too large. Please upload a video smaller than 1 GB.');
                        input.value = '';
                        return false;
                    }
                    return true;
                }                  
                                  
                function validateForm() {
                    const videoInput = document.querySelector('input[name="input_video"]');
                    if (!videoInput.files || videoInput.files.length === 0) {
                        alert('Please upload a video file.');
                        return false;
                    }
                    return true;
                }                  
                
                
                // Event listeners
                document.addEventListener('DOMContentLoaded', function() {
                    document.getElementById('uploadForm').addEventListener('submit', function(e) {
                        if (!validateForm()) {
                            e.preventDefault();
                        }
                    });
                });

            </script>
        </body>
        </html>
    """)

@app.route('/upload_files', methods=['POST'])
def upload_files():
    """Handles file uploads and sets up the paths."""
    global uploaded_voice_samples, uploaded_input_video, setup_complete, user_system_prompt
    uploaded_voice_samples = []
    setup_upload_directories()

    # 1. Get the video file from the request at the beginning.
    video_file = request.files.get('input_video')

    # 2. Consolidate the logic into a single block.
    if video_file and video_file.filename:
        filename = secure_filename(video_file.filename)
        filepath = os.path.join(INPUT_VIDEO_DIR, filename)
        video_file.save(filepath)
        uploaded_input_video = filepath # Set the global variable here

        # Now, perform the audio extraction using the saved filepath
        extracted_audio_path = os.path.join(VOICE_SAMPLES_DIR, EXTRACTED_AUDIO_PATH)
        if extract_audio_from_video(filepath, extracted_audio_path):
            isolated_voice_path = os.path.join(VOICE_SAMPLES_DIR, ISOLATED_VOICE_PATH)
            if isolate_voice_from_audio(extracted_audio_path, isolated_voice_path):
                uploaded_voice_samples = [isolated_voice_path]
                print("✅ Voice extraction and isolation completed successfully!")
            else:
                uploaded_voice_samples = [extracted_audio_path]
                print("⚠️ Voice isolation failed, using raw extracted audio")
        else:
            return "Failed to extract audio from video. Please ensure the video has an audio track.", 400
    else:
        return "Missing video file. Please upload a video.", 400

    # Get the system prompt from the form and combine with base prompt
    today_date = date.today().strftime("%B %d, %Y")
    date_prompt = f"For your information, today's date is {today_date}."
    user_custom_prompt = request.form.get('system_prompt', '').strip()
    if user_custom_prompt:
        user_system_prompt = f"{BASE_SYSTEM_PROMPT} {date_prompt} {user_custom_prompt}"
    else:
        user_system_prompt = f"{BASE_SYSTEM_PROMPT} {date_prompt}"

    if not uploaded_voice_samples or not uploaded_input_video:
        return "Missing files or failed to process video. Please try uploading a different video file.", 40

    setup_complete = True
    return render_template_string("""
        <!doctype html>
        <html lang="en">
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Upload Successful</title>
            <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
            <style>
                * {
                    margin: 0;
                    padding: 0;
                    box-sizing: border-box;
                }
                
                body {
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    min-height: 100vh;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    padding: 20px;
                }
                
                .success-container {
                    background: rgba(255, 255, 255, 0.95);
                    backdrop-filter: blur(10px);
                    border-radius: 20px;
                    padding: 60px 40px;
                    box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
                    max-width: 500px;
                    width: 100%;
                    text-align: center;
                    border: 1px solid rgba(255, 255, 255, 0.2);
                    animation: successPulse 2s ease-in-out;
                }
                
                .success-icon {
                    font-size: 4rem;
                    color: #48bb78;
                    margin-bottom: 20px;
                    animation: bounce 1s ease-in-out;
                }
                
                h1 {
                    color: #2d3748;
                    font-size: 2rem;
                    font-weight: 700;
                    margin-bottom: 15px;
                }
                
                p {
                    color: #4a5568;
                    font-size: 1.1rem;
                    line-height: 1.6;
                    margin-bottom: 30px;
                }
                
                .loading-dots {
                    display: inline-block;
                }
                
                .loading-dots::after {
                    content: '';
                    animation: dots 2s steps(4, end) infinite;
                }
                
                @keyframes dots {
                    0%, 20% { content: ''; }
                    40% { content: '.'; }
                    60% { content: '..'; }
                    80%, 100% { content: '...'; }
                }
                
                @keyframes successPulse {
                    0% {
                        transform: scale(0.8);
                        opacity: 0;
                    }
                    50% {
                        transform: scale(1.05);
                    }
                    100% {
                        transform: scale(1);
                        opacity: 1;
                    }
                }
                
                @keyframes bounce {
                    0%, 20%, 50%, 80%, 100% {
                        transform: translateY(0);
                    }
                    40% {
                        transform: translateY(-20px);
                    }
                    60% {
                        transform: translateY(-10px);
                    }
                }
            </style>
        </head>
        <body>
            <div class="success-container">
                <div class="success-icon">
                    <i class="fas fa-check-circle"></i>
                </div>
                <h1>Upload Successful!</h1>
                <p>Your interactive clone is being prepared with your custom personality<span class="loading-dots"></span></p>
                <p><strong>Check your terminal for the next steps!</strong></p>
            </div>
        </body>
        </html>
    """)

def run_flask_app():
    """Runs the Flask web server."""
    print("Starting web interface on http://127.0.0.1:5000")
    app.run(debug=False, use_reloader=False)

# --- NEW: Helper Functions for Interaction ---

def overlay_icon(background_img, icon_img):
    """
    Overlays a smaller icon onto the bottom-right of a background image.
    Assumes icon_img is a BGRA image (with transparency).
    """
    bg_h, bg_w, _ = background_img.shape
    icon_h, icon_w, _ = icon_img.shape

    # --- MODIFIED: Scale icon to be 1/8 of the background's width (making it smaller) ---
    scale_factor = (bg_w / 8) / icon_w
    new_w = int(icon_w * scale_factor)
    new_h = int(icon_h * scale_factor)
    resized_icon = cv2.resize(icon_img, (new_w, new_h))

    # --- MODIFIED: Position in the bottom right corner with padding ---
    padding = 20  # pixels from the edge
    x_offset = bg_w - new_w - padding
    y_offset = bg_h - new_h - padding

    # Separate alpha channel and BGR channels
    alpha = resized_icon[:, :, 3] / 255.0
    bgr_icon = resized_icon[:, :, :3]
    
    # Create an inverse alpha mask
    inv_alpha = 1.0 - alpha

    # Select Region of Interest (ROI) from the background
    roi = background_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w]

    # Blend the icon and the ROI
    blended_roi = (bgr_icon * alpha[..., np.newaxis] + roi * inv_alpha[..., np.newaxis]).astype(background_img.dtype)

    # Place the blended ROI back onto the background
    result_img = background_img.copy()
    result_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = blended_roi
    
    return result_img

def remove_background_advanced(image_path, output_path):
    """
    Advanced background removal focusing on preserving people's faces and bodies.
    Uses multiple techniques: person detection, skin detection, and improved segmentation.
    """
    # Read the image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Could not load image: {image_path}")
        return False
    
    height, width = img.shape[:2]
    
    try:
        # Method 1: Try MediaPipe Selfie Segmentation (most accurate for people)
        if try_mediapipe_segmentation(img, output_path):
            print(f"Background removed using MediaPipe and saved to: {output_path}")
            return True
    except:
        print("MediaPipe not available, using alternative methods...")
    
    # Method 2: Enhanced GrabCut with person detection
    person_mask = detect_person_region(img)
    
    if person_mask is not None:
        # Use detected person region for better GrabCut initialization
        result = enhanced_grabcut_with_person_detection(img, person_mask)
    else:
        # Method 3: Fallback to skin detection + improved GrabCut
        result = enhanced_grabcut_with_skin_detection(img)
    
    # Save the result
    cv2.imwrite(output_path, result)
    print(f"Background removed and saved to: {output_path}")
    return True

def isolate_voice_from_audio(input_audio_path, output_audio_path):
    """
    Extract and isolate voice from audio using audio-separator library.
    """
    try:
        from audio_separator.separator import Separator
        
        print("🎵 Isolating voice from audio using AI...")
        
        # Initialize the separator with a vocals isolation model
        separator = Separator(
            log_level=logging.WARNING,  # Reduce console output
            log_formatter=None,
            model_file_dir='./models',  # Cache models locally
            output_dir=os.path.dirname(output_audio_path),
            output_format='wav'
        )
        
        # Load a model optimized for vocal isolation
        # UVR_MDXNET_KARA_2 is excellent for vocal isolation
        separator.load_model(model_filename='UVR_MDXNET_KARA_2.onnx')
        
        # Separate the audio - this returns paths to the separated files
        output_files = separator.separate(input_audio_path)

        # 🔍 DEBUG: Let's see what's really happening
        print("=" * 60)
        print("🔍 DEBUG INFO:")
        print(f"Input file: {input_audio_path}")
        print(f"Expected output: {output_audio_path}")
        print(f"Output dir set to: {os.path.dirname(output_audio_path)}")
        print(f"Separator returned: {output_files}")
        print(f"Number of files returned: {len(output_files) if output_files else 0}")

        # Check what's actually in the output directory
        output_dir = os.path.dirname(output_audio_path)
        if os.path.exists(output_dir):
            actual_files = os.listdir(output_dir)
            print(f"Actual files in {output_dir}: {actual_files}")
        else:
            print(f"Output directory doesn't exist: {output_dir}")

        # Check each returned file
        if output_files:
            for i, file_path in enumerate(output_files):
                print(f"File {i}: {file_path}")
                print(f"  Exists: {os.path.exists(file_path)}")
                if os.path.exists(file_path):
                    print(f"  Size: {os.path.getsize(file_path)} bytes")
                
        print("=" * 60)
        
        # The separator typically outputs "vocals" and "instrumental" files
        # Find the vocals file and rename it to our desired output path
        # Find the vocals file and rename it to our desired output path
        vocals_file = None
        output_dir = os.path.dirname(output_audio_path)

        for file_path in output_files:
            # Build the full path since separator returns relative filenames
            full_path = os.path.join(output_dir, file_path)
            if os.path.exists(full_path) and 'vocals' in file_path.lower():
                vocals_file = full_path
                break
        
        if vocals_file and os.path.exists(vocals_file):
            # Move/rename the vocals file to our desired location
            shutil.move(vocals_file, output_audio_path)
            
            # Clean up any remaining files
            for file_path in output_files:
                if os.path.exists(file_path) and file_path != vocals_file:
                    os.remove(file_path)
            
            print(f"✅ Voice isolation completed. Saved to: {output_audio_path}")
            return True
        else:
            print("❌ Could not find vocals output file")
            return False
            
    except Exception as e:
        print(f"❌ Voice isolation failed: {e}")
        return False
    
def extract_audio_from_video(video_path, output_audio_path):
    """
    Extract audio from video file using ffmpeg.
    """
    try:
        print("🎬 Extracting audio from video...")
        
        cmd = [
            'ffmpeg', '-y',  # -y to overwrite output files
            '-i', video_path,  # input video
            '-vn',  # no video
            '-acodec', 'pcm_s16le',  # audio codec
            '-ar', '22050',  # sample rate
            '-ac', '1',  # mono
            output_audio_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print(f"✅ Audio extracted successfully to: {output_audio_path}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"❌ Audio extraction failed: {e}")
        print("Make sure ffmpeg is installed and accessible from command line")
        return False
    except Exception as e:
        print(f"❌ Unexpected error during audio extraction: {e}")
        return False

def extract_first_frame(video_path, output_path):
    """Extract the first frame from a video and save as image."""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {video_path}")
        return False
    
    ret, frame = cap.read()
    cap.release()
    
    if ret:
        cv2.imwrite(output_path, frame)
        return True
    else:
        print("Error: Could not read first frame from video")
        return False
    
def get_video_duration(video_path):
    """Get the duration of a video in seconds."""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return 0
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps if fps > 0 else 0
    cap.release()
    return duration

def get_audio_duration(audio_path):
    """Get the duration of an audio file in seconds."""
    import wave
    try:
        with wave.open(audio_path, 'rb') as audio_file:
            frames = audio_file.getnframes()
            sample_rate = audio_file.getframerate()
            duration = frames / sample_rate
            return duration
    except:
        # Fallback method using subprocess
        import subprocess
        try:
            result = subprocess.run(['ffprobe', '-v', 'quiet', '-show_entries', 
                                   'format=duration', '-of', 'csv=p=0', audio_path], 
                                   capture_output=True, text=True)
            return float(result.stdout.strip())
        except:
            return 0

def loop_video_to_match_audio(video_path, audio_duration, output_path):
    """Loop video to match audio duration."""
    import subprocess
    
    video_duration = get_video_duration(video_path)
    if video_duration == 0:
        return False
    
    # Calculate how many times to loop
    loop_count = int(audio_duration / video_duration) + 1
    
    # Use ffmpeg to loop the video
    cmd = [
        'ffmpeg', '-y', '-stream_loop', str(loop_count), '-i', video_path,
        '-t', str(audio_duration), '-c', 'copy', output_path
    ]
    
    try:
        subprocess.run(cmd, capture_output=True, check=True)
        return True
    except subprocess.CalledProcessError:
        print("Error: ffmpeg not available. Cannot loop video.")
        return False

def try_mediapipe_segmentation(img, output_path):
    """Use rembg with GPU acceleration for better background removal."""
    try:
        from rembg import remove, new_session
        
        # Force GPU usage if available
        if torch.cuda.is_available():
            session = new_session('u2net', providers=['CUDAExecutionProvider'])
        else:
            session = new_session('u2net')
        
        # Convert OpenCV BGR to PIL RGB
        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(rgb_img)
        
        # Remove background with GPU session
        output_pil = remove(pil_img, session=session)
        
        # Convert back to OpenCV format
        result_array = np.array(output_pil)
        if result_array.shape[2] == 4:  # RGBA
            alpha = result_array[:, :, 3] / 255.0
            result = result_array[:, :, :3] * alpha[:, :, np.newaxis]
        else:
            result = result_array
        
        # Convert RGB back to BGR for OpenCV
        result = cv2.cvtColor(result.astype(np.uint8), cv2.COLOR_RGB2BGR)
        
        cv2.imwrite(output_path, result)
        return True
        
    except Exception as e:
        print(f"rembg segmentation failed: {e}")
        return False

def detect_person_region(img):
    """
    Detect person/human regions using HOG descriptor or Haar cascades.
    """
    try:
        # Initialize HOG descriptor for person detection
        hog = cv2.HOGDescriptor()
        hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
        
        # Detect people
        boxes, weights = hog.detectMultiScale(img, winStride=(8, 8), padding=(32, 32), scale=1.05)
        
        if len(boxes) > 0:
            # Create mask from detected person regions
            mask = np.zeros(img.shape[:2], dtype=np.uint8)
            
            for (x, y, w, h) in boxes:
                # Expand the bounding box slightly
                x = max(0, x - 20)
                y = max(0, y - 20)
                w = min(img.shape[1] - x, w + 40)
                h = min(img.shape[0] - y, h + 40)
                
                mask[y:y+h, x:x+w] = 255
            
            return mask
            
    except Exception as e:
        print(f"Person detection failed: {e}")
        
    return None

def detect_skin_regions(img):
    """
    Detect skin regions to help identify people.
    """
    # Convert to HSV and YCrCb color spaces for better skin detection
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)
    
    # Skin color ranges in HSV
    lower_hsv = np.array([0, 20, 70], dtype=np.uint8)
    upper_hsv = np.array([20, 255, 255], dtype=np.uint8)
    
    # Skin color ranges in YCrCb
    lower_ycrcb = np.array([0, 135, 85], dtype=np.uint8)
    upper_ycrcb = np.array([255, 180, 135], dtype=np.uint8)
    
    # Create masks
    mask_hsv = cv2.inRange(hsv, lower_hsv, upper_hsv)
    mask_ycrcb = cv2.inRange(ycrcb, lower_ycrcb, upper_ycrcb)
    
    # Combine masks
    skin_mask = cv2.bitwise_and(mask_hsv, mask_ycrcb)
    
    # Clean up the mask
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))
    skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)
    skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
    
    return skin_mask

def enhanced_grabcut_with_person_detection(img, person_mask):
    """
    Enhanced GrabCut using detected person regions.
    """
    # Create initial mask for GrabCut
    mask = np.zeros(img.shape[:2], np.uint8)
    
    # Set person regions as probable foreground
    mask[person_mask == 255] = cv2.GC_PR_FGD
    mask[person_mask == 0] = cv2.GC_BGD
    
    # GrabCut models
    bgdModel = np.zeros((1, 65), np.float64)
    fgdModel = np.zeros((1, 65), np.float64)
    
    # Apply GrabCut
    cv2.grabCut(img, mask, None, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_MASK)
    
    # Create final mask
    final_mask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')
    
    # Post-process mask
    final_mask = refine_mask(final_mask, img)
    
    # Apply mask to image
    result = img * final_mask[:, :, np.newaxis]
    
    return result

def enhanced_grabcut_with_skin_detection(img):
    """
    Enhanced GrabCut using skin detection and improved initialization.
    """
    height, width = img.shape[:2]
    
    # Detect skin regions
    skin_mask = detect_skin_regions(img)
    
    # Create initial GrabCut mask
    mask = np.zeros(img.shape[:2], np.uint8)
    
    # Use a more conservative rectangle (center 70% of image)
    rect = (int(width * 0.15), int(height * 0.05), int(width * 0.7), int(height * 0.9))
    
    # GrabCut models
    bgdModel = np.zeros((1, 65), np.float64)
    fgdModel = np.zeros((1, 65), np.float64)
    
    # Initial GrabCut with rectangle
    cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 3, cv2.GC_INIT_WITH_RECT)
    
    # Refine with skin detection
    if np.sum(skin_mask) > 1000:  # If significant skin regions detected
        # Set skin regions as probable foreground
        mask[skin_mask == 255] = cv2.GC_PR_FGD
        
        # Continue GrabCut with mask
        cv2.grabCut(img, mask, None, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_MASK)
    
    # Create final mask
    final_mask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')
    
    # Post-process mask
    final_mask = refine_mask(final_mask, img)
    
    # Apply mask to image
    result = img * final_mask[:, :, np.newaxis]
    
    return result

def refine_mask(mask, img):
    """
    Refine the segmentation mask to remove noise and improve edges.
    """
    # Remove small noise
    kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_small)
    
    # Fill small holes
    kernel_large = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_large)
    
    # Find largest connected component (assumed to be the person)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, 8, cv2.CV_32S)
    
    if num_labels > 1:
        # Find the largest component (excluding background)
        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
        mask = (labels == largest_label).astype(np.uint8)
    
    # Smooth the edges
    mask = cv2.GaussianBlur(mask.astype(np.float32), (3, 3), 0)
    mask = (mask > 0.5).astype(np.uint8)
    
    return mask

def remove_background_from_video(input_video_path, output_video_path):
    """
    Remove background from an entire video using rembg with GPU acceleration.
    """
    print(f"Removing background from video: {input_video_path}")
    
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {input_video_path}")
        return False
    
    # Get video properties
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # Define codec and create VideoWriter
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
    
    frame_count = 0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    try:
        from rembg import remove, new_session
        from PIL import Image
        import numpy as np
        
        # Create GPU session once for better performance
        if torch.cuda.is_available():
            session = new_session('u2net', providers=['CUDAExecutionProvider'])
            print("Using GPU acceleration for background removal")
        else:
            session = new_session('u2net')
            print("Using CPU for background removal")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            if frame_count % 30 == 0:  # Progress update every 30 frames
                print(f"Processing frame {frame_count}/{total_frames}")
            
            # Convert BGR to RGB for PIL
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_frame = Image.fromarray(rgb_frame)
            
            # Remove background using rembg
            output_pil = remove(pil_frame, session=session)
            
            # Convert back to OpenCV format
            result_array = np.array(output_pil)
            
            if result_array.shape[2] == 4:  # RGBA
                # Use alpha channel to create clean background removal
                alpha = result_array[:, :, 3] / 255.0
                result_frame = result_array[:, :, :3] * alpha[:, :, np.newaxis]
                result_frame = cv2.cvtColor(result_frame.astype(np.uint8), cv2.COLOR_RGB2BGR)
            else:
                # Fallback if no alpha channel
                result_frame = cv2.cvtColor(result_array, cv2.COLOR_RGB2BGR)
            
            # Write the frame
            out.write(result_frame)
                
    except Exception as e:
        print(f"Error during video background removal: {e}")
        cap.release()
        out.release()
        return False
    
    cap.release()
    out.release()
    print(f"Background removal completed. Output saved to: {output_video_path}")
    return True

def listen_for_speech():
    """Captures audio from the microphone and converts it to text."""
    r = sr.Recognizer()
    with sr.Microphone() as source:
        print("🎤 Listening...")
        r.pause_threshold = 1.0 # seconds of non-speaking audio before phrase is considered complete
        r.adjust_for_ambient_noise(source)
        try:
            audio = r.listen(source, timeout=10)
        except sr.WaitTimeoutError:
            print("No speech detected.")
            return None

    try:
        print("Recognizing...")
        text = r.recognize_google(audio)
        return text
    except sr.UnknownValueError:
        print("Google Speech Recognition could not understand audio.")
        return None
    except sr.RequestError as e:
        print(f"Could not request results from Google Speech Recognition service; {e}")
        return None
    
def remove_emojis(text):
    """Remove all emojis and special Unicode characters from text."""
    # Comprehensive emoji pattern that covers most emoji ranges
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002702-\U000027B0"  # dingbats
        "\U000024C2-\U0001F251"  # enclosed characters
        "\U0001F900-\U0001F9FF"  # supplemental symbols
        "\U0001FA70-\U0001FAFF"  # symbols and pictographs extended-A
        "\U00002600-\U000026FF"  # miscellaneous symbols
        "\U00002700-\U000027BF"  # dingbats
        "]+", 
        flags=re.UNICODE
    )
    
    # Remove emojis
    text = emoji_pattern.sub('', text)
    
    # Remove other problematic Unicode characters and normalize spaces
    text = re.sub(r'[^\x00-\x7F]+', '', text)  # Remove non-ASCII
    text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
    text = text.strip()
    
    return text

def get_ollama_response(prompt, conversation_history=None, base_img=None, internet_icon=None):
    """Sends a prompt to the Ollama API and gets a response, with optional internet search."""
    
    # Check if the query needs internet search
    search_context = ""
    if needs_internet_ml(prompt):
        print("🌐 Query requires internet search. Searching...")
        
        # Show internet icon if provided
        if base_img is not None and internet_icon is not None:
            img_with_internet = overlay_icon(base_img, internet_icon)
            cv2.imshow(DISPLAY_WINDOW_NAME, img_with_internet)
            cv2.waitKey(1)
        
        search_results = web_search(prompt)
        if search_results and "No results found" not in search_results:
            search_context = f"\n\nCurrent search results for '{prompt}':\n{search_results}\n\nPlease use this information to provide an accurate, up-to-date response."
            print(f"✅ Search completed. Found relevant information.")
        else:
            print("❌ Search completed but no relevant results found.")
    
    # Show thinking icon before Ollama processing
    print("🧠 Getting response from Ollama...")
    if base_img is not None:
        # Load thinking icon here since we need it in this function now
        thinking_icon = cv2.imread(THINKING_ICON_PATH, cv2.IMREAD_UNCHANGED)
        if thinking_icon is not None:
            img_with_thinking = overlay_icon(base_img, thinking_icon)
            cv2.imshow(DISPLAY_WINDOW_NAME, img_with_thinking)
            cv2.waitKey(1)
    
    # Use the combined system prompt (base + user custom)
    current_system_prompt = user_system_prompt if user_system_prompt else BASE_SYSTEM_PROMPT
    
    # Build the full prompt with system prompt, conversation history, and current query
    full_prompt = current_system_prompt + "\n\n"
    
    # Add conversation history if provided
    if conversation_history:
        for message in conversation_history[-10:]:  # Keep last 10 messages for context
            if message["role"] == "user":
                full_prompt += f"Human: {message['content']}\n"
            else:
                full_prompt += f"Assistant: {message['content']}\n"
    
    # Add the current user message with search context if available
    user_message = prompt + search_context
    full_prompt += f"Human: {user_message}\nAssistant: "
    
    # Ollama API payload
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": full_prompt,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 150
        }
    }
    
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(OLLAMA_API_URL, json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        response_data = response.json()
        
        # Extract the response from Ollama format
        if "response" in response_data:
            ai_response = response_data["response"].strip()
            # Remove emojis from the AI response
            clean_response = remove_emojis(ai_response)
            return clean_response
        else:
            print("Unexpected response format from Ollama")
            return "I seem to be having trouble thinking right now."
            
    except requests.exceptions.RequestException as e:
        print(f"Error connecting to Ollama API: {e}")
        print("Please ensure Ollama is running on port 11434 with a model loaded.")
        return "I seem to be having trouble thinking right now."
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response: {e}")
        return "I seem to be having trouble thinking right now."
    except Exception as e:
        print(f"Unexpected error: {e}")
        return "I seem to be having trouble thinking right now."

# --- Original Script Functions (modified) ---

def play_video_and_revert(video_path, image_path):
    """
    Plays video with audio, synchronized using a master clock from Pygame
    to prevent timing drift.
    """
    print(f"Playing video with robust, clock-synced audio: {video_path}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {video_path}")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0:
        print("Warning: Could not get video FPS. Defaulting to 30.")
        fps = 30

    pygame.init()
    try:
        pygame.mixer.init()
        pygame.mixer.music.load(OUTPUT_WAV_PATH)
        pygame.mixer.music.play()
    except Exception as e:
        print(f"Error initializing Pygame or loading audio: {e}. Video will play without sound.")
        # Fallback to a simple silent video player
        wait_ms = int(1000 / fps)
        while True:
            ret, frame = cap.read()
            if not ret: break
            cv2.imshow(DISPLAY_WINDOW_NAME, frame)
            if cv2.waitKey(wait_ms) & 0xFF == ord('q'): break
        cap.release()
        if 'pygame' in sys.modules and pygame.get_init():
             pygame.quit()
        # Revert image
        base_img = cv2.imread(image_path)
        if base_img is not None:
            cv2.imshow(DISPLAY_WINDOW_NAME, base_img)
            cv2.waitKey(1)
        return

    # --- Master Clock Synchronization Loop ---
    # Get the time that playback starts
    playback_start_time = pygame.time.get_ticks()

    while True:
        ret, frame = cap.read()
        if not ret:
            break # Video has finished

        # Get the timestamp of the video frame we just read (in milliseconds)
        video_timestamp = cap.get(cv2.CAP_PROP_POS_MSEC)

        # We now wait until the master clock has caught up to the video's timestamp
        while True:
            # Get the actual time that has passed since playback started
            elapsed_time = pygame.time.get_ticks() - playback_start_time
            if elapsed_time >= video_timestamp:
                break # It's time to show the frame

        cv2.imshow(DISPLAY_WINDOW_NAME, frame)

        # A minimal waitKey is still needed to process OpenCV GUI events
        if cv2.waitKey(1) & 0xFF == ord('q'):
            pygame.mixer.music.stop()
            break
        
        # Also check if the music has stopped playing
        if not pygame.mixer.music.get_busy():
            break

    # --- Cleanup ---
    cap.release()
    if 'pygame' in sys.modules and pygame.get_init():
        pygame.quit()

    print("Video playback completed.")
    
    # Revert to base image
    base_img = cv2.imread(image_path)
    if base_img is not None:
        cv2.imshow(DISPLAY_WINDOW_NAME, base_img)
        cv2.waitKey(1)

# --- NEW: Internet Search Configuration ---
internet_training_questions = [
    "What's the weather like today?",
    "Who won the latest football game?",
    "What are the current stock prices?",
    "What's your favorite color?",
    "How are you feeling today?",
    "Tell me about yourself",
    "What's happening in the news today?",
    "Do you like music?",
    "What's the current time in Tokyo?",
    "What are the latest tech updates?",
    "What's 2 plus 2?",
    "How do you work?",
    "What's your name?",
    "What's the latest on climate change?",
    "Who is the current president?",
    "Can you help me with math?",
    "What are today's top headlines?",
    "What do you think about AI?",
    "How old are you?",
    "What's your purpose?",
    "Are you human?"
]

internet_training_labels = [
    1, # needs internet
    1,
    1,
    0, # local answer
    0,
    0,
    1,
    0,
    1,
    1,
    0,
    0,
    0, # personal question, no internet needed
    1,
    1,
    0,
    1,
    0,
    0,
    0,
    0
]

# Initialize ML model for internet detection
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(internet_training_questions)
ML_model = LogisticRegression()
ML_model.fit(X_train, internet_training_labels)

def needs_internet_ml(query):
    """Determine if a query needs internet search using ML."""
    query_vec = vectorizer.transform([query])
    prediction = ML_model.predict(query_vec)[0]
    return prediction == 1

def web_search(query, max_results=4):
    """
    Drop-in replacement for DuckDuckGo search using Bing
    Returns same format as original function
    """
    try:
        # Add delay to avoid rate limiting
        time.sleep(random.uniform(1, 2))
        
        # Headers to mimic a real browser
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # URL encode the query
        encoded_query = urllib.parse.quote_plus(query)
        url = f"https://www.bing.com/search?q={encoded_query}"
        
        # Make the request
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Parse the HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        snippets = []
        results_found = 0
        
        # Find search results (Bing uses different CSS selectors)
        for result in soup.find_all('li', class_='b_algo'):
            if results_found >= max_results:
                break
                
            # Extract title
            title_elem = result.find('h2')
            if title_elem:
                title_link = title_elem.find('a')
                if title_link:
                    title = title_link.get_text(strip=True)
                    href = title_link.get('href', '')
                else:
                    title = title_elem.get_text(strip=True)
                    href = ''
            else:
                title = ''
                href = ''
            
            # Extract body/description
            body_elem = result.find('div', class_='b_caption')
            if body_elem:
                # Remove any nested elements we don't want
                for unwanted in body_elem.find_all(['strong', 'em']):
                    unwanted.unwrap()
                body = body_elem.get_text(strip=True)
            else:
                body = ''
            
            # Only add if we have meaningful content
            if title and body:
                snippets.append(f"{title}\n{body}\n{href}\n")
                results_found += 1
        
        return "\n".join(snippets) if snippets else "No results found."
        
    except requests.exceptions.RequestException as e:
        return f"Request failed: {str(e)}"
    except Exception as e:
        return f"Search failed: {str(e)}"
    
def send_visca(cmd_bytes):
    global ser
    try:
        if ser and ser.is_open:
            ser.write(bytes(cmd_bytes + [0xFF]))
            time.sleep(0.01)
    except (serial.SerialException, OSError) as e:
        print(f"Serial communication error: {e}")
        # Don't close ser here as it might be used by other parts

def pan_tilt_command(x_dir, y_dir):
    send_visca([0x81, 0x01, 0x06, 0x01, PAN_SPEED, TILT_SPEED, x_dir, y_dir])

def stop_motion():
    pan_tilt_command(0x03, 0x03)

def start_face_tracking():
    def tracking_loop():
        global tracking_active
        mp_face = mp.solutions.face_detection
        face = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5)
        FRAME_WIDTH = 640
        FRAME_HEIGHT = 480
        CENTER_X = FRAME_WIDTH // 2
        CENTER_Y = FRAME_HEIGHT // 2
        TOLERANCE = 40
        
        while tracking_active:  # Changed from True to tracking_active
            if tracking_paused:
                time.sleep(0.1)
                continue
                
            with camera_lock:
                frame = latest_frame.copy() if latest_frame is not None else None
                
            if frame is None:
                time.sleep(0.05)  # Add small delay when no frame
                continue
                
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face.process(rgb)
            
            if results.detections:
                detection = results.detections[0]
                bbox = detection.location_data.relative_bounding_box
                x = int(bbox.xmin * FRAME_WIDTH)
                y = int(bbox.ymin * FRAME_HEIGHT)
                w = int(bbox.width * FRAME_WIDTH)
                h = int(bbox.height * FRAME_HEIGHT)
                
                face_center_x = x + w // 2
                face_center_y = y + h // 2
                dx = face_center_x - CENTER_X
                dy = face_center_y - CENTER_Y
                
                x_dir = 0x03
                y_dir = 0x03
                
                if dx < -TOLERANCE:
                    x_dir = 0x02
                elif dx > TOLERANCE:
                    x_dir = 0x01
                    
                if dy < -TOLERANCE:
                    y_dir = 0x01
                elif dy > TOLERANCE:
                    y_dir = 0x02
                    
                # Check if tracking is still active before sending commands
                if tracking_active and ser and ser.is_open:
                    if x_dir == 0x03 and y_dir == 0x03:
                        stop_motion()
                    else:
                        pan_tilt_command(x_dir, y_dir)
            else:
                # Check if tracking is still active before sending stop command
                if tracking_active and ser and ser.is_open:
                    stop_motion()
                
            time.sleep(0.05)
        
        print("Face tracking thread stopped cleanly")
    
    threading.Thread(target=tracking_loop, daemon=True).start()

def main():
    """Main function to run the voice cloning and interactive session."""
    global uploaded_voice_samples, uploaded_input_video, setup_complete, conversation_history, user_system_prompt, tracking_active    
    print("🚀 Starting the Interactive Voice Clone with Ollama! 🚀")
    
    flask_thread = threading.Thread(target=run_flask_app)
    flask_thread.daemon = True
    flask_thread.start()

    print("Please open your web browser to http://127.0.0.1:5000 to upload files.")
    while not setup_complete:
        time.sleep(1)

    print("\n✅ Files uploaded! Initializing models...")

    # --- NEW: Remove background from the entire video ---
    processed_video_path = os.path.join(INPUT_VIDEO_DIR, "processed_video.mp4")
    video_to_use = uploaded_input_video # Default to original if removal fails

    print("🎬 Starting video background removal process...")
    if remove_background_from_video(uploaded_input_video, processed_video_path):
        print("✅ Background removal successful. Using processed video.")
        video_to_use = processed_video_path
    else:
        print("⚠️ Background removal failed. Proceeding with the original video.")
    # --- END OF NEW SECTION ---

    # Initialize serial connection for face tracking
    global ser
    for port in POSSIBLE_PORTS:
        try:
            ser = serial.Serial(port, BAUD, timeout=1)
            print(f"✔️ Successfully connected to serial port: {port}")
            break
        except Exception as e:
            print(f"❌ Failed to connect to {port}: {e}")
    if ser is None:
        print("❌ Could not establish serial connection. Face tracking will be disabled.")

    # Initialize camera for face tracking
    global camera_cap
    try:
        camera_cap = cv2.VideoCapture(0)
        if camera_cap.isOpened():
            print("✔️ Camera initialized for face tracking")
            # Start camera frame capture thread
            def camera_thread():
                global latest_frame
                while True:
                    ret, frame = camera_cap.read()
                    if ret:
                        with camera_lock:
                            latest_frame = frame
                    time.sleep(0.033)  # ~30 fps
            
            threading.Thread(target=camera_thread, daemon=True).start()
        else:
            print("❌ Could not initialize camera for face tracking")
            camera_cap = None
    except Exception as e:
        print(f"❌ Camera initialization failed: {e}")
        camera_cap = None

    # Start face tracking if both serial and camera are available
    if ser and camera_cap:
        start_face_tracking()
        print("✔️ Face tracking started")

    # --- 1. Set Up Devices (Smart Device Management) ---
    # Use MPS for video processing but CPU for TTS
    video_device = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.cuda.is_available():
        print(f"Using device: {video_device.upper()}")

    # --- 2. Load TTS Model ---
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"🚀 Loading Chatterbox-TTS model on {device.upper()}...")
    tts = ChatterboxTTS.from_pretrained(device=device)

    # --- 4. Load Icons and Base Image ---
    # --- 4. Extract First Frame and Load Icons ---
    first_frame_path = "first_frame.jpg"
    if not extract_first_frame(video_to_use, first_frame_path):
        print(f"Failed to extract first frame from {uploaded_input_video}. Exiting.")
        return

    base_img = cv2.imread(first_frame_path)

    if base_img is None:
        print(f"Failed to load photo from {first_frame_path}. Exiting.")
        return

    mic_icon = cv2.imread(MIC_ICON_PATH, cv2.IMREAD_UNCHANGED)
    if mic_icon is None:
        print(f"ERROR: Could not load mic icon from '{MIC_ICON_PATH}'.")
        print("Please ensure the file exists in the 'icons' folder.")
        return

    thinking_icon = cv2.imread(THINKING_ICON_PATH, cv2.IMREAD_UNCHANGED)
    if thinking_icon is None:
        print(f"ERROR: Could not load thinking icon from '{THINKING_ICON_PATH}'.")
        return
    
    # Load internet icon
    internet_icon = cv2.imread("icons/internet.png", cv2.IMREAD_UNCHANGED)
    if internet_icon is None:
        print(f"ERROR: Could not load internet icon from 'icons/internet.png'.")
        return

    # Load speech synthesis icon  
    speech_synthesis_icon = cv2.imread("icons/speech-synthesis.png", cv2.IMREAD_UNCHANGED)
    if speech_synthesis_icon is None:
        print(f"ERROR: Could not load speech synthesis icon from 'icons/speech-synthesis.png'.")
        return
    
    # Load video generation icon
    video_generation_icon = cv2.imread("icons/video-generation.png", cv2.IMREAD_UNCHANGED)
    if video_generation_icon is None:
        print(f"ERROR: Could not load video generation icon from 'icons/video-generation.png'.")
        return
        
    # --- 4. Main Interaction Loop ---
    
    print(f"\nAttemping to show window....")
    cv2.namedWindow(DISPLAY_WINDOW_NAME, cv2.WINDOW_NORMAL)
    print(f"\nAttemping to show window2....")
    cv2.imshow(DISPLAY_WINDOW_NAME, base_img)
    
    print("\n" + "="*50)
    print("✅ Setup complete! The clone is ready.")
    print("Press the SPACEBAR in the clone's window to talk.")
    print("Press 'q' in the window to quit the program.")
    print("Make sure Ollama is running with the right model installed!")
    print("="*50 + "\n")

    from lipsync import LipSync

    # --- 3. Initialize LipSync Model ---
    os.makedirs('cache', exist_ok=True)
    print("🎬 Loading LipSync model...")
    try:
        lip = LipSync(
            model='wav2lip',
            checkpoint_path='weights/wav2lip_gan.pth',
            nosmooth=True,
            device=video_device,  # Use the same device as video processing
            cache_dir='cache',
            img_size=96,
            save_cache=True,
        )
        print(f"\n✅ LipSync model loaded")
        print(f"LipSync using device: {video_device}")
        if torch.cuda.is_available():
            print(f"CUDA device: {torch.cuda.get_device_name()}")
            print(f"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
    except Exception as e:
        print(f"❌ Failed to load LipSync model: {e}")
        return

    while True:
        key = cv2.waitKey(0) & 0xFF
        if key == ord('q'):
            print("Shutting down...")
            # First stop the tracking thread
            tracking_active = False
            time.sleep(0.2)  # Give the thread time to stop
            break
        if key == ord(' '):
            # State 1: Listening
            img_with_mic = overlay_icon(base_img, mic_icon)
            cv2.imshow(DISPLAY_WINDOW_NAME, img_with_mic)
            cv2.waitKey(1)
            
            user_prompt = listen_for_speech()

            if user_prompt:
                print(f"💬 You said: {user_prompt}")

                # State 2: Processing (icons will be shown inside get_ollama_response)
                TEXT_TO_SPEAK = get_ollama_response(user_prompt, conversation_history, base_img, internet_icon)
                
                if TEXT_TO_SPEAK:
                    print(f"🤖 Clone says: {TEXT_TO_SPEAK}")
                    
                    # Add to conversation history for context
                    conversation_history.append({
                        "role": "user",
                        "content": user_prompt
                    })
                    conversation_history.append({
                        "role": "assistant", 
                        "content": TEXT_TO_SPEAK
                    })
                    
                    # Keep conversation history manageable (last 10 exchanges)
                    if len(conversation_history) > 20:  # 10 user + 10 assistant messages
                        conversation_history = conversation_history[-20:]
                    
                    # State 3: Speech Synthesis
                    try:
                        # Show speech synthesis icon
                        img_with_speech = overlay_icon(base_img, speech_synthesis_icon)
                        cv2.imshow(DISPLAY_WINDOW_NAME, img_with_speech)
                        cv2.waitKey(1)
                        
                        print("Generating speech with TTS...")
                        
                        # Use safe TTS generation
                        try:
                            # Use the first uploaded voice sample as the audio prompt
                            wav = tts.generate(TEXT_TO_SPEAK, audio_prompt_path=uploaded_voice_samples[0])
                            ta.save(OUTPUT_WAV_PATH, wav, tts.sr)
                        except Exception as tts_error:
                            print(f"TTS error: {tts_error}")
                            # Retry
                            wav = tts.generate(TEXT_TO_SPEAK, audio_prompt_path=uploaded_voice_samples[0])
                            ta.save(OUTPUT_WAV_PATH, wav, tts.sr)
                        
                        # State 4: Video Generation with LipSync
                        print("🎬 Generating lip-synced video...")
                        img_with_video_gen = overlay_icon(base_img, video_generation_icon)
                        cv2.imshow(DISPLAY_WINDOW_NAME, img_with_video_gen)
                        cv2.waitKey(1)

                        # Generate lip-synced video
                        synced_video_path = "synced_output.mp4"
                        try:
                            lip.sync(
                                video_to_use,
                                OUTPUT_WAV_PATH,
                                synced_video_path,
                            )
                            print("✅ Lip-sync video generation completed.")
                            
                            # Play the synced video (it already has the audio embedded)
                            print("Playing lip-synced video...")
                            play_video_and_revert(synced_video_path, first_frame_path)
                            
                        except Exception as e:
                            print(f"❌ Lip-sync generation failed: {e}")
                            print("Falling back to original video with generated audio...")
                            play_video_and_revert(video_to_use, first_frame_path)

                    except Exception as e:
                        print(f"An error occurred during TTS or Wav2Lip: {e}")
                        cv2.imshow(DISPLAY_WINDOW_NAME, base_img)
                else:
                    print("Ollama returned an empty response.")
                    cv2.imshow(DISPLAY_WINDOW_NAME, base_img)
            else:
                print("Could not understand you. Please try again.")
                cv2.imshow(DISPLAY_WINDOW_NAME, base_img)
    
    # --- 5. Cleanup ---
    print("Cleaning up...")
    
    # Stop face tracking thread first
    tracking_active = False
    time.sleep(0.5)  # Give thread time to exit cleanly
    
    # Stop any ongoing motion before closing serial
    if ser and ser.is_open:
        try:
            stop_motion()
            time.sleep(0.1)
        except Exception as e:
            print(f"Error stopping motion during cleanup: {e}")
        finally:
            ser.close()
            print("Serial connection closed")
    
    # Cleanup camera
    if camera_cap:
        camera_cap.release()
        print("Camera released")
    
    # Close OpenCV windows
    cv2.destroyAllWindows()
    
    # Clean up temporary files
    if os.path.exists(UPLOAD_FOLDER):
        print("Cleaning up temporary files...")
        shutil.rmtree(UPLOAD_FOLDER)
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    
    print("Program finished cleanly.")


if __name__ == "__main__":
    setup_upload_directories()
    main()